{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a700f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import fitz  # pymupdf\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dd0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Directories for your project\n",
    "BASE_DIR = os.getcwd()\n",
    "PDF_DIR = os.path.join(BASE_DIR, 'pdfs')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'rag_data')\n",
    "INDEX_DIR = os.path.join(DATA_DIR, 'faiss_index')\n",
    "META_PATH = os.path.join(DATA_DIR, 'metadata.pkl')\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8569b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akib Hasan\\AppData\\Local\\Temp\\ipykernel_22684\\2840802507.py:4: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceBgeEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# Load the BAAI embedding model\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs={'device': 'cpu'}, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6fb490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00e3993bb0e40d7a27a3fdbf540a161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading PDFs:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_text_from_pdf(path: str) -> str:\n",
    "    \"\"\"Extract plain text from a PDF file using PyMuPDF (fitz).\"\"\"\n",
    "    text_parts = []\n",
    "    doc = fitz.open(path)\n",
    "    for page_no in range(len(doc)):\n",
    "        page = doc.load_page(page_no)\n",
    "        page_text = page.get_text(\"text\")\n",
    "        if page_text:\n",
    "            page_text = \"\\n\".join([line.strip() for line in page_text.splitlines() if line.strip()])\n",
    "            text_parts.append(page_text)\n",
    "    doc.close()\n",
    "    return \"\\n\\n\".join(text_parts)\n",
    "\n",
    "pdf_paths = sorted(glob.glob(os.path.join(PDF_DIR, \"*.pdf\")))\n",
    "all_docs = []\n",
    "for p in tqdm(pdf_paths, desc='Reading PDFs'):\n",
    "    txt = extract_text_from_pdf(p)\n",
    "    doc = {\n",
    "        'id': os.path.basename(p),\n",
    "        'text': txt,\n",
    "        'source': p\n",
    "    }\n",
    "    all_docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddf0438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 800, overlap: int = 200) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    avg_word_len = max(1, sum(len(w) for w in tokens) / len(tokens))  # Average word length\n",
    "    words_per_chunk = max(50, int(chunk_size / avg_word_len))  # Number of words per chunk\n",
    "    overlap_words = max(10, int(overlap / avg_word_len))  # Number of words to overlap\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(len(tokens), start + words_per_chunk)\n",
    "        chunk = \" \".join(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start = end - overlap_words\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c6e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_docs = []\n",
    "for doc in all_docs:  # Assuming all_docs contains a list of documents, each with 'id' and 'text'\n",
    "    chunks = chunk_text(doc['text'])\n",
    "    for i, c in enumerate(chunks):\n",
    "        chunked_docs.append({\n",
    "            'chunk_id': f\"{doc['id']}_chunk_{i}\",\n",
    "            'text': c,\n",
    "            'source': doc['id'],\n",
    "            'chunk_index': i\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eddba260",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [c['text'] for c in chunked_docs]  # Extracting the text from chunked_docs\n",
    "embeddings = embeddings_model.embed_documents(texts)  # Generate embeddings for each chunk\n",
    "emb_matrix = np.array(embeddings, dtype='float32')  # Converting the embeddings into a numpy array\n",
    "faiss.normalize_L2(emb_matrix)  # Normalizing the embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee3ddcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FAISS index using Inner Product (IP)\n",
    "index = faiss.IndexFlatIP(emb_matrix.shape[1])\n",
    "\n",
    "# Add the embeddings to the index\n",
    "index.add(emb_matrix)\n",
    "\n",
    "# Save the FAISS index to disk\n",
    "faiss.write_index(index, os.path.join(INDEX_DIR, 'faiss.index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6a207e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the chunked documents (metadata) to pickle for later use\n",
    "with open(META_PATH, 'wb') as f:\n",
    "    pickle.dump(chunked_docs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35422558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to a file (optional, to load later in Streamlit)\n",
    "with open('embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(emb_matrix, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c17a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
